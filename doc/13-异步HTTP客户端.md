# ç¬¬13è¯¾ï¼šå¼‚æ­¥ HTTP å®¢æˆ·ç«¯

## ğŸ¯ æœ¬è¯¾ç›®æ ‡

- ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
- å­¦ä¹  httpx çš„ä½¿ç”¨æ–¹æ³•
- æŒæ¡è¶…æ—¶å’Œé”™è¯¯å¤„ç†

---

## ğŸ“– ä¸ºä»€ä¹ˆç”¨ httpxï¼Ÿ

### requests vs httpx

| ç‰¹æ€§ | requests | httpx |
|------|----------|-------|
| åŒæ­¥æ”¯æŒ | âœ… | âœ… |
| å¼‚æ­¥æ”¯æŒ | âŒ | âœ… |
| HTTP/2 | âŒ | âœ… |
| ç°ä»£åŒ– | è¾ƒè€ | æ–°è®¾è®¡ |

### å¼‚æ­¥çš„ä¼˜åŠ¿

```python
# åŒæ­¥æ–¹å¼ - ä¸²è¡Œæ‰§è¡Œï¼Œç­‰å¾…é˜»å¡
response1 = requests.get(url1)  # ç­‰å¾…...
response2 = requests.get(url2)  # ç­‰å¾…...
# æ€»æ—¶é—´ = æ—¶é—´1 + æ—¶é—´2

# å¼‚æ­¥æ–¹å¼ - å¹¶å‘æ‰§è¡Œ
response1, response2 = await asyncio.gather(
    client.get(url1),
    client.get(url2)
)
# æ€»æ—¶é—´ â‰ˆ max(æ—¶é—´1, æ—¶é—´2)
```

---

## ğŸ” LLMService å¼‚æ­¥è¯·æ±‚

æŸ¥çœ‹ `app/services/llm.py`ï¼š

### æœåŠ¡ç±»åˆå§‹åŒ–

```python
class LLMService:
    """LLM API æœåŠ¡"""
    
    def __init__(
        self,
        api_base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        timeout: Optional[int] = None,
        max_retries: int = 2
    ):
        settings = get_settings()
        
        self.api_base_url = api_base_url or settings.llm_api_base_url
        self.api_key = api_key or settings.llm_api_key
        self.model = model or settings.llm_model
        self.timeout = timeout or settings.llm_timeout
        self.max_retries = max_retries
        
        # ç§»é™¤æœ«å°¾æ–œæ 
        self.api_base_url = self.api_base_url.rstrip("/")
```

### æ„å»ºè¯·æ±‚å¤´

```python
def _get_headers(self) -> dict[str, str]:
    """è·å– HTTP è¯·æ±‚å¤´"""
    headers = {
        "Content-Type": "application/json",
    }
    if self.api_key:
        headers["Authorization"] = f"Bearer {self.api_key}"
    return headers
```

### æ„å»ºè¯·æ±‚ä½“

```python
def _build_request_body(
    self,
    messages: list[dict[str, str]],
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> dict[str, Any]:
    """æ„å»º API è¯·æ±‚ä½“"""
    body = {
        "model": self.model,
        "messages": messages,
        "temperature": temperature,
    }
    
    if max_tokens is not None:
        body["max_tokens"] = max_tokens
        
    return body
```

---

## ğŸ“ å¼‚æ­¥è¯·æ±‚å®ç°

```python
async def _make_request(
    self,
    messages: list[dict[str, str]],
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> dict[str, Any]:
    """å‘é€å¼‚æ­¥ HTTP è¯·æ±‚"""
    
    url = f"{self.api_base_url}/chat/completions"
    headers = self._get_headers()
    body = self._build_request_body(messages, temperature, max_tokens)
    
    # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    async with httpx.AsyncClient(timeout=self.timeout) as client:
        try:
            # å‘é€ POST è¯·æ±‚
            response = await client.post(
                url, 
                headers=headers, 
                json=body
            )
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    error_detail = error_json.get("error", {}).get("message", error_detail)
                except:
                    pass
                
                raise LLMAPIError(
                    f"LLM API error: {error_detail}",
                    status_code=response.status_code
                )
            
            return response.json()
            
        except httpx.TimeoutException as e:
            logger.error(f"LLM API timeout: {e}")
            raise LLMTimeoutError(
                f"Request timed out after {self.timeout} seconds"
            )
        except httpx.RequestError as e:
            logger.error(f"LLM API request error: {e}")
            raise LLMAPIError(f"Request failed: {str(e)}")
```

### å…³é”®ç‚¹è§£æ

```python
# 1. å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async with httpx.AsyncClient(timeout=30) as client:
    # client ä¼šåœ¨é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­

# 2. å¼‚æ­¥ POST è¯·æ±‚
response = await client.post(url, headers=headers, json=body)

# 3. json å‚æ•°ä¼šè‡ªåŠ¨
#    - å°† dict åºåˆ—åŒ–ä¸º JSON
#    - è®¾ç½® Content-Type: application/json

# 4. å¼‚å¸¸å¤„ç†
# TimeoutException - è¶…æ—¶
# RequestError - ç½‘ç»œé”™è¯¯
```

---

## ğŸ”„ é‡è¯•æœºåˆ¶

```python
async def chat(
    self,
    messages: list[dict[str, str]],
    system_prompt: str,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> tuple[str, dict[str, int]]:
    """å‘é€èŠå¤©è¯·æ±‚ï¼Œæ”¯æŒé‡è¯•"""
    
    formatted_messages = format_messages_for_llm(messages, system_prompt)
    last_error = None
    
    # é‡è¯•å¾ªç¯
    for attempt in range(self.max_retries + 1):
        try:
            response_data = await self._make_request(
                formatted_messages,
                temperature,
                max_tokens
            )
            return parse_llm_response(response_data)
            
        except LLMTimeoutError:
            # è¶…æ—¶ä¸é‡è¯•
            raise
            
        except LLMAPIError as e:
            last_error = e
            # åªé‡è¯• 5xx æœåŠ¡å™¨é”™è¯¯
            if e.status_code and 500 <= e.status_code < 600:
                if attempt < self.max_retries:
                    logger.warning(f"Retry {attempt + 1}/{self.max_retries}")
                    continue
            # 4xx å®¢æˆ·ç«¯é”™è¯¯ä¸é‡è¯•
            raise
            
        except Exception as e:
            last_error = e
            if attempt < self.max_retries:
                logger.warning(f"Unexpected error, retrying: {e}")
                continue
            raise LLMAPIError(f"Unexpected error: {str(e)}")
    
    if last_error:
        raise last_error
```

### é‡è¯•ç­–ç•¥

```
è¯·æ±‚å¤±è´¥
    â”‚
    â–¼
åˆ¤æ–­é”™è¯¯ç±»å‹
    â”‚
    â”œâ”€â”€ TimeoutException â†’ ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡º
    â”‚
    â”œâ”€â”€ 5xx æœåŠ¡å™¨é”™è¯¯ â†’ é‡è¯•ï¼ˆæœ€å¤š N æ¬¡ï¼‰
    â”‚
    â”œâ”€â”€ 4xx å®¢æˆ·ç«¯é”™è¯¯ â†’ ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡º
    â”‚
    â””â”€â”€ å…¶ä»–é”™è¯¯ â†’ é‡è¯•ï¼ˆæœ€å¤š N æ¬¡ï¼‰
```

---

## ğŸ“Š httpx å¸¸ç”¨æ“ä½œ

### åŸºæœ¬è¯·æ±‚

```python
import httpx

# GET è¯·æ±‚
async with httpx.AsyncClient() as client:
    response = await client.get("https://api.example.com/data")
    data = response.json()

# POST è¯·æ±‚ï¼ˆJSONï¼‰
async with httpx.AsyncClient() as client:
    response = await client.post(
        "https://api.example.com/data",
        json={"key": "value"}
    )

# POST è¯·æ±‚ï¼ˆè¡¨å•ï¼‰
async with httpx.AsyncClient() as client:
    response = await client.post(
        "https://api.example.com/form",
        data={"field": "value"}
    )
```

### è®¾ç½®è¶…æ—¶

```python
# å…¨å±€è¶…æ—¶
client = httpx.AsyncClient(timeout=30.0)

# ç»†ç²’åº¦è¶…æ—¶
timeout = httpx.Timeout(
    connect=5.0,    # è¿æ¥è¶…æ—¶
    read=30.0,      # è¯»å–è¶…æ—¶
    write=10.0,     # å†™å…¥è¶…æ—¶
    pool=10.0       # è¿æ¥æ± è¶…æ—¶
)
client = httpx.AsyncClient(timeout=timeout)
```

### è®¾ç½®è¯·æ±‚å¤´

```python
headers = {
    "Authorization": "Bearer token",
    "User-Agent": "MyApp/1.0"
}
async with httpx.AsyncClient(headers=headers) as client:
    response = await client.get(url)
```

---

## ğŸ“ åŠ¨æ‰‹ç»ƒä¹ 

### ç»ƒä¹ ï¼šæ·»åŠ è¯·æ±‚æ—¥å¿—

```python
import logging
import time

logger = logging.getLogger(__name__)

async def _make_request_with_logging(self, ...):
    """å¸¦æ—¥å¿—çš„è¯·æ±‚"""
    
    start_time = time.time()
    logger.info(f"Calling LLM API: {self.model}")
    
    try:
        response = await self._make_request(...)
        elapsed = time.time() - start_time
        logger.info(f"LLM response received in {elapsed:.2f}s")
        return response
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"LLM request failed after {elapsed:.2f}s: {e}")
        raise
```

---

## ğŸ“ æœ¬è¯¾å°ç»“

| çŸ¥è¯†ç‚¹ | æŒæ¡ç¨‹åº¦ |
|--------|---------|
| ç†è§£å¼‚æ­¥ HTTP çš„ä¼˜åŠ¿ | â˜ |
| ä¼šä½¿ç”¨ httpx.AsyncClient | â˜ |
| ç†è§£è¶…æ—¶é…ç½® | â˜ |
| ä¼šå¤„ç†å„ç±»å¼‚å¸¸ | â˜ |
| ç†è§£é‡è¯•ç­–ç•¥ | â˜ |

---

## ğŸ”œ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬14è¯¾ï¼šå¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†** - å­¦ä¹ å¦‚ä½•ç®¡ç†å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚

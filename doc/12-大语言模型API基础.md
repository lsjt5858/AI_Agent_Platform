# ç¬¬12è¯¾ï¼šå¤§è¯­è¨€æ¨¡å‹ API åŸºç¡€

## ğŸ¯ æœ¬è¯¾ç›®æ ‡

- äº†è§£ OpenAI å…¼å®¹ API çš„æ¥å£è§„èŒƒ
- ç†è§£ Chat Completion API çš„ä½¿ç”¨æ–¹æ³•
- å­¦ä¹ æ¶ˆæ¯æ ¼å¼å’Œå‚æ•°é…ç½®

---

## ğŸ“– ä»€ä¹ˆæ˜¯ LLM APIï¼Ÿ

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰API å…è®¸æˆ‘ä»¬é€šè¿‡ HTTP è¯·æ±‚ä¸ AI æ¨¡å‹äº¤äº’ã€‚

### å¸¸è§çš„ LLM æœåŠ¡

| æœåŠ¡ | æä¾›å•† | ç‰¹ç‚¹ |
|------|--------|------|
| é€šä¹‰åƒé—® | é˜¿é‡Œäº‘ | å›½å†…è®¿é—®ç¨³å®šï¼Œä»·æ ¼è¾ƒä½ |
| OpenAI | OpenAI | æ¨¡å‹èƒ½åŠ›å¼ºï¼Œéœ€ç§‘å­¦ä¸Šç½‘ |
| DeepSeek | DeepSeek | æ€§ä»·æ¯”é«˜ï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡ |
| æ–‡å¿ƒä¸€è¨€ | ç™¾åº¦ | å›½å†…æœåŠ¡ï¼Œä¸­æ–‡è¾ƒå¥½ |

### OpenAI å…¼å®¹æ¥å£

å¤§å¤šæ•° LLM æœåŠ¡éƒ½æä¾›ä¸ OpenAI å…¼å®¹çš„ API æ¥å£ï¼Œæ ¼å¼ç»Ÿä¸€ï¼š

```
POST /v1/chat/completions
```

---

## ğŸ” API è¯·æ±‚æ ¼å¼

### è¯·æ±‚ç»“æ„

```python
POST https://api.example.com/v1/chat/completions

Headers:
    Authorization: Bearer sk-xxxxx
    Content-Type: application/json

Body:
{
    "model": "qwen-turbo",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "ä½ å¥½"}
    ],
    "temperature": 0.7,
    "max_tokens": 2000
}
```

### å…³é”®å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `model` | str | æ¨¡å‹åç§°ï¼Œå¦‚ "qwen-turbo" |
| `messages` | list | å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ |
| `temperature` | float | éšæœºæ€§ï¼Œ0-2ï¼Œè¶Šé«˜è¶Šéšæœº |
| `max_tokens` | int | æœ€å¤§è¾“å‡º token æ•° |

### æ¶ˆæ¯è§’è‰²

| è§’è‰² | ç”¨é€” |
|------|------|
| `system` | ç³»ç»Ÿæç¤ºï¼Œå®šä¹‰ AI çš„è¡Œä¸ºå’Œäººè®¾ |
| `user` | ç”¨æˆ·å‘é€çš„æ¶ˆæ¯ |
| `assistant` | AI çš„å›å¤ |

---

## ğŸ“ å“åº”æ ¼å¼

```json
{
    "id": "chatcmpl-xxxxx",
    "object": "chat.completion",
    "created": 1234567890,
    "model": "qwen-turbo",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 20,
        "completion_tokens": 15,
        "total_tokens": 35
    }
}
```

### å“åº”å­—æ®µè¯´æ˜

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `choices[0].message.content` | AI çš„å›å¤å†…å®¹ |
| `choices[0].finish_reason` | ç»“æŸåŸå› ï¼ˆstop/length/...ï¼‰|
| `usage.prompt_tokens` | è¾“å…¥æ¶ˆè€—çš„ token |
| `usage.completion_tokens` | è¾“å‡ºæ¶ˆè€—çš„ token |
| `usage.total_tokens` | æ€» token æ¶ˆè€— |

---

## ğŸ”§ é¡¹ç›®ä¸­çš„ LLM å°è£…

æŸ¥çœ‹ `app/services/llm.py`ï¼š

### æ ¼å¼åŒ–æ¶ˆæ¯

```python
def format_messages_for_llm(
    messages: list[dict[str, str]],
    system_prompt: str
) -> list[dict[str, str]]:
    """
    å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸º LLM API éœ€è¦çš„æ ¼å¼
    """
    formatted = []
    
    # 1. æ·»åŠ ç³»ç»Ÿæç¤º
    if system_prompt:
        formatted.append({
            "role": "system",
            "content": system_prompt
        })
    
    # 2. æ·»åŠ å¯¹è¯æ¶ˆæ¯
    for msg in messages:
        formatted.append({
            "role": msg.get("role", "user"),
            "content": msg.get("content", "")
        })
    
    return formatted
```

**ç¤ºä¾‹å˜æ¢**ï¼š

è¾“å…¥ï¼š
```python
messages = [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}
]
system_prompt = "ä½ æ˜¯ä¸€ä¸ªå¤©æ°”åŠ©æ‰‹"
```

è¾“å‡ºï¼š
```python
[
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¤©æ°”åŠ©æ‰‹"},
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼"},
    {"role": "user", "content": "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"}
]
```

### è§£æå“åº”

```python
def parse_llm_response(response_data: dict) -> tuple[str, dict[str, int]]:
    """
    è§£æ LLM API å“åº”
    
    Returns:
        (å›å¤å†…å®¹, tokenä½¿ç”¨ç»Ÿè®¡)
    """
    try:
        choices = response_data.get("choices", [])
        if not choices:
            raise LLMAPIError("No choices in response")
        
        # æå–å›å¤å†…å®¹
        message = choices[0].get("message", {})
        content = message.get("content")
        
        if content is None:
            raise LLMAPIError("No content in message")
        
        # æå– token ä½¿ç”¨
        usage = response_data.get("usage", {})
        token_usage = {
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        }
        
        return content, token_usage
        
    except (KeyError, IndexError) as e:
        raise LLMAPIError(f"Failed to parse response: {e}")
```

---

## ğŸ“Š Temperature å‚æ•°è¯¦è§£

```
Temperature = 0.0 (ç¡®å®šæ€§)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŒæ ·çš„è¾“å…¥ï¼Œæ¯æ¬¡è¾“å‡ºå‡ ä¹ç›¸åŒ           â”‚
â”‚ é€‚åˆï¼šäº‹å®æŸ¥è¯¢ã€ä»£ç ç”Ÿæˆ               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temperature = 0.7 (å¹³è¡¡ï¼Œé»˜è®¤)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åœ¨è´¨é‡å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡             â”‚
â”‚ é€‚åˆï¼šä¸€èˆ¬å¯¹è¯ã€é—®ç­”                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temperature = 1.5+ (é«˜åˆ›é€ æ€§)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¾“å‡ºæ›´åŠ éšæœºã€æœ‰åˆ›æ„                   â”‚
â”‚ é€‚åˆï¼šåˆ›æ„å†™ä½œã€å¤´è„‘é£æš´               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ cURL æµ‹è¯•ç¤ºä¾‹

```bash
curl https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-turbo",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
      {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python"}
    ]
  }'
```

---

## ğŸ“‹ Token è®¡è´¹è¯´æ˜

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| Token | æ–‡æœ¬çš„æœ€å°å•ä½ï¼Œä¸­æ–‡çº¦1å­—=1token |
| prompt_tokens | è¾“å…¥æ¶ˆè€—çš„ tokenï¼ˆåŒ…æ‹¬å†å²æ¶ˆæ¯ï¼‰|
| completion_tokens | è¾“å‡ºæ¶ˆè€—çš„ token |
| è®¡è´¹ | é€šå¸¸æŒ‰ token æ•°é‡è®¡è´¹ |

### Token ä¼°ç®—

| è¯­è¨€ | å¤§çº¦æ¯”ä¾‹ |
|------|---------|
| è‹±æ–‡ | 1 token â‰ˆ 4 ä¸ªå­—ç¬¦ |
| ä¸­æ–‡ | 1 token â‰ˆ 1-2 ä¸ªå­—ç¬¦ |

---

## ğŸ”’ API Key å®‰å…¨

```python
# âŒ é”™è¯¯ï¼šç¡¬ç¼–ç åœ¨ä»£ç ä¸­
api_key = "sk-xxxxxxxxxxxx"

# âœ… æ­£ç¡®ï¼šä»ç¯å¢ƒå˜é‡è¯»å–
import os
api_key = os.environ.get("LLM_API_KEY")

# âœ… æ›´å¥½ï¼šä½¿ç”¨é…ç½®ç®¡ç†
from .config import get_settings
settings = get_settings()
api_key = settings.llm_api_key
```

---

## ğŸ“ æœ¬è¯¾å°ç»“

| çŸ¥è¯†ç‚¹ | æŒæ¡ç¨‹åº¦ |
|--------|---------|
| ç†è§£ OpenAI å…¼å®¹ API æ ¼å¼ | â˜ |
| ç†è§£æ¶ˆæ¯è§’è‰²ï¼ˆsystem/user/assistantï¼‰| â˜ |
| ç†è§£ temperature å‚æ•°ä½œç”¨ | â˜ |
| ç†è§£ token æ¦‚å¿µå’Œè®¡è´¹ | â˜ |
| ä¼šä½¿ç”¨ cURL æµ‹è¯• API | â˜ |

---

## ğŸ”œ ä¸‹ä¸€è¯¾é¢„å‘Š

**ç¬¬13è¯¾ï¼šå¼‚æ­¥ HTTP å®¢æˆ·ç«¯** - å­¦ä¹ ä½¿ç”¨ httpx è¿›è¡Œå¼‚æ­¥ API è°ƒç”¨ã€‚

# 第14课：对话上下文管理

## 🎯 本课目标

- 理解多轮对话的上下文机制
- 学习历史消息的存储和检索
- 掌握上下文窗口的管理策略

---

## 📖 什么是对话上下文？

LLM 本身是**无状态**的，每次请求都是独立的。为了实现多轮对话，需要在每次请求时带上历史消息。

### 对比

**无上下文**：
```
用户：我叫小明
AI：你好，小明！
用户：我叫什么？
AI：抱歉，我不知道您的名字。  ← 😢 不记得了
```

**有上下文**：
```
用户：我叫小明
AI：你好，小明！
用户：我叫什么？
AI：你叫小明呀！  ← 😊 记住了
```

---

## 🔍 上下文传递流程

```
用户发送消息
      │
      ▼
┌─────────────────────────────────────────────────────┐
│                  MessageService                      │
│                                                      │
│  1. 从数据库获取历史消息                               │
│     SELECT * FROM messages                           │
│     WHERE conversation_id = ?                        │
│     ORDER BY created_at                              │
│                                                      │
│  2. 构建消息上下文                                    │
│     [                                                │
│       {"role": "user", "content": "我叫小明"},        │
│       {"role": "assistant", "content": "你好，小明"}, │
│       {"role": "user", "content": "我叫什么？"}       │  ← 新消息
│     ]                                                │
│                                                      │
│  3. 添加系统提示                                      │
│     [                                                │
│       {"role": "system", "content": "你是助手"},      │  ← 系统提示
│       {"role": "user", "content": "我叫小明"},        │
│       {"role": "assistant", "content": "你好，小明"}, │
│       {"role": "user", "content": "我叫什么？"}       │
│     ]                                                │
│                                                      │
│  4. 发送给 LLM API                                   │
│                                                      │
└─────────────────────────────────────────────────────┘
      │
      ▼
LLM 返回：你叫小明呀！
```

---

## 📝 代码实现

### 获取历史消息

```python
# repositories/message.py

async def get_by_conversation(
    self, 
    conversation_id: int
) -> List[Message]:
    """获取对话的所有消息，按时间排序"""
    result = await self.session.execute(
        select(Message)
        .where(Message.conversation_id == conversation_id)
        .order_by(Message.created_at.asc())  # 按时间升序
    )
    return list(result.scalars().all())
```

### 构建消息上下文

```python
# services/message.py

def _build_message_context(
    self, 
    messages: List[Message]
) -> List[dict]:
    """将 Message 对象转为 LLM 格式"""
    return [
        {"role": msg.role, "content": msg.content}
        for msg in messages
    ]
```

### 完整的发送流程

```python
async def send_message(
    self, 
    conversation_id: int, 
    content: str
) -> tuple[Message, Message]:
    """发送消息并获取 AI 回复"""
    
    # 1. 验证对话存在
    conversation = await self.conversation_repository.get_by_id(
        conversation_id
    )
    if conversation is None:
        raise ConversationNotFoundError(conversation_id)
    
    # 2. 获取 Agent 的系统提示
    agent = await self.agent_repository.get_by_id(conversation.agent_id)
    system_prompt = agent.system_prompt if agent else "You are helpful."
    
    # 3. 保存用户消息到数据库
    user_message = await self.message_repository.create(
        conversation_id=conversation_id,
        role="user",
        content=content
    )
    
    # 4. 获取完整的历史消息（包括刚保存的）
    messages = await self.message_repository.get_by_conversation(
        conversation_id
    )
    
    # 5. 转换为 LLM 格式
    message_context = self._build_message_context(messages)
    
    # 6. 调用 LLM
    ai_response, token_usage = await self.llm_service.chat(
        messages=message_context,
        system_prompt=system_prompt
    )
    
    # 7. 保存 AI 回复到数据库
    assistant_message = await self.message_repository.create(
        conversation_id=conversation_id,
        role="assistant",
        content=ai_response
    )
    
    # 8. 提交事务
    await self.session.commit()
    
    return user_message, assistant_message
```

---

## 📊 上下文窗口限制

### 问题

LLM 有上下文长度限制（如 4K、8K、128K tokens）。历史消息太长会：
1. 超出模型限制
2. 消耗大量 tokens（费用增加）
3. 降低响应速度

### 解决策略

#### 策略1：滑动窗口

```python
def _build_message_context_with_limit(
    self, 
    messages: List[Message],
    max_messages: int = 20
) -> List[dict]:
    """限制最大消息数量"""
    # 只保留最近的 N 条消息
    recent_messages = messages[-max_messages:]
    return [
        {"role": msg.role, "content": msg.content}
        for msg in recent_messages
    ]
```

#### 策略2：Token 限制

```python
def _build_message_context_with_token_limit(
    self, 
    messages: List[Message],
    max_tokens: int = 3000
) -> List[dict]:
    """限制总 token 数"""
    result = []
    total_tokens = 0
    
    # 从最新消息开始，倒序添加
    for msg in reversed(messages):
        # 估算 token 数（中文约 1 字符 = 1 token）
        msg_tokens = len(msg.content)
        
        if total_tokens + msg_tokens > max_tokens:
            break
            
        result.insert(0, {"role": msg.role, "content": msg.content})
        total_tokens += msg_tokens
    
    return result
```

#### 策略3：消息压缩/摘要

```python
async def _summarize_old_messages(
    self, 
    messages: List[Message]
) -> str:
    """将旧消息压缩为摘要"""
    old_messages = messages[:-10]  # 除最近10条外
    
    if not old_messages:
        return ""
    
    # 让 LLM 生成摘要
    summary = await self.llm_service.chat(
        messages=[{
            "role": "user", 
            "content": f"请简要总结以下对话内容：\n{old_messages}"
        }],
        system_prompt="你是一个摘要助手"
    )
    
    return summary
```

---

## 🔧 数据流图

```
        数据库                    内存                    LLM API
          │                        │                        │
          │   ┌────────────────────┼────────────────────┐   │
          │   │     发送消息请求                         │   │
          │   └────────────────────┼────────────────────┘   │
          │                        │                        │
          │   ┌────────────────────┴────────────────────┐   │
          │◀──┤ 1. 读取历史消息                          │   │
          │   └────────────────────┬────────────────────┘   │
          │                        │                        │
          │   ┌────────────────────┴────────────────────┐   │
          │◀──┤ 2. 保存用户消息                          │   │
          │   └────────────────────┬────────────────────┘   │
          │                        │                        │
          │   ┌────────────────────┴────────────────────┐   │
          │   │ 3. 构建上下文                            │──▶│
          │   │    [system, user, assistant, user...]   │   │
          │   └────────────────────┬────────────────────┘   │
          │                        │                        │
          │                        │◀──────────────────────│
          │   ┌────────────────────┴────────────────────┐   │
          │   │ 4. 接收 AI 回复                          │   │
          │   └────────────────────┬────────────────────┘   │
          │                        │                        │
          │   ┌────────────────────┴────────────────────┐   │
          │◀──┤ 5. 保存 AI 回复                          │   │
          │   └────────────────────┬────────────────────┘   │
          │                        │                        │
```

---

## 🎓 动手练习

### 练习：实现消息数量限制

```python
# 在 MessageService 中添加

MAX_CONTEXT_MESSAGES = 20  # 最大上下文消息数

async def send_message(self, conversation_id: int, content: str):
    # ... 前面的代码 ...
    
    # 获取历史消息并限制数量
    messages = await self.message_repository.get_by_conversation(conversation_id)
    
    # 只保留最近 N 条
    if len(messages) > MAX_CONTEXT_MESSAGES:
        messages = messages[-MAX_CONTEXT_MESSAGES:]
    
    message_context = self._build_message_context(messages)
    
    # ... 后续代码 ...
```

---

## 📝 本课小结

| 知识点 | 掌握程度 |
|--------|---------|
| 理解 LLM 的无状态特性 | ☐ |
| 理解上下文传递机制 | ☐ |
| 会存储和检索历史消息 | ☐ |
| 理解上下文窗口限制 | ☐ |
| 了解常见的上下文管理策略 | ☐ |

---

## 🔜 下一课预告

**第15课：配置管理最佳实践** - 学习如何优雅地管理应用配置。
